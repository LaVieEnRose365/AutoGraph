{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import set_seed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "set_seed(42)\n",
    "dataset = \"ml-1m\"\n",
    "source_dir = os.path.join(f\"Datasets/{dataset}\", \"raw_data\")\n",
    "target_dir = os.path.join(f\"data/{dataset}\", \"proc_data\")\n",
    "os.makedirs(target_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_dict = {\n",
    "    1: \"under 18\",\n",
    "    18: \"18-24\",\n",
    "    25: \"25-34\",\n",
    "    35: \"35-44\",\n",
    "    45: \"45-49\",\n",
    "    50: \"50-55\",\n",
    "    56: \"above 56\"\n",
    "}\n",
    "\n",
    "job_dict = {\n",
    "    0: \"other or not specified\",\n",
    "\t1: \"academic/educator\",\n",
    "\t2: \"artist\",\n",
    "\t3: \"clerical/admin\",\n",
    "\t4: \"college/grad student\",\n",
    "\t5: \"customer service\",\n",
    "\t6: \"doctor/health care\",\n",
    "\t7: \"executive/managerial\",\n",
    "\t8: \"farmer\",\n",
    "\t9: \"homemaker\",\n",
    "\t10: \"K-12 student\",\n",
    "\t11: \"lawyer\",\n",
    "\t12: \"programmer\",\n",
    "\t13: \"retired\",\n",
    "\t14: \"sales/marketing\",\n",
    "\t15: \"scientist\",\n",
    "\t16: \"self-employed\",\n",
    "\t17: \"technician/engineer\",\n",
    "\t18: \"tradesman/craftsman\",\n",
    "\t19: \"unemployed\",\n",
    "\t20: \"writer\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User data\n",
    "\n",
    "user_data = []\n",
    "user_fields = [\"User ID\", \"Gender\", \"Age\", \"Job\", \"Zipcode\"]\n",
    "for line in open(os.path.join(source_dir, \"users.dat\"), \"r\").readlines():\n",
    "    ele = line.strip().split(\"::\")\n",
    "    user_id, gender, age, job, zipcode = [x.strip() for x in ele]\n",
    "    assert gender in [\"M\", \"F\"], ele\n",
    "    gender = \"male\" if gender == \"M\" else \"female\"\n",
    "    age = age_dict[int(age)]\n",
    "    job = job_dict[int(job)]\n",
    "    user_data.append([user_id, gender, age, job, zipcode])\n",
    "\n",
    "df_user = pd.DataFrame(user_data, columns=user_fields)\n",
    "print(f\"Total number of users: {len(df_user)}\")\n",
    "assert len(df_user[\"User ID\"]) == len(set(df_user[\"User ID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie data\n",
    "\n",
    "movie_data = []\n",
    "movie_fields = [\"Movie ID\", \"Movie title\", \"Movie genre\"]\n",
    "for line in open(os.path.join(source_dir, \"movies.dat\"), \"r\", encoding=\"ISO-8859-1\").readlines():\n",
    "    ele = line.strip().split(\"::\")\n",
    "    movie_id = ele[0].strip()\n",
    "    movie_title = ele[1].strip()\n",
    "    movie_genre = ele[2].strip().split(\"|\")[0]\n",
    "    movie_data.append([movie_id, movie_title, movie_genre])\n",
    "\n",
    "df_movie = pd.DataFrame(movie_data, columns=movie_fields)\n",
    "print(f\"Total number of movies: {len(df_movie)}\")\n",
    "assert len(df_movie[\"Movie ID\"]) == len(set(df_movie[\"Movie ID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating data\n",
    "\n",
    "rating_data = []\n",
    "rating_fields = [\"User ID\", \"Movie ID\", \"rating\", \"timestamp\"]\n",
    "user_list = list(df_user[\"User ID\"])\n",
    "for line in open(os.path.join(source_dir, \"ratings.dat\"), \"r\").readlines():\n",
    "    ele = [x.strip() for x in line.strip().split(\"::\")] \n",
    "    user, movie, rating, timestamp = ele[0], ele[1], int(ele[2]), int(ele[3])\n",
    "    if user in user_list:\n",
    "        rating_data.append([user, movie, rating, timestamp])\n",
    "\n",
    "df_ratings = pd.DataFrame(rating_data, columns=rating_fields)\n",
    "print(f\"Total number of ratings: {len(df_ratings)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.merge(df_ratings, df_user, on=[\"User ID\"], how=\"inner\")\n",
    "df_data = pd.merge(df_data, df_movie, on=[\"Movie ID\"], how=\"inner\")\n",
    "\n",
    "df_data = df_data[df_data[\"rating\"] > 3]\n",
    "\n",
    "df_data.sort_values(by=[\"timestamp\", \"User ID\", \"Movie ID\"], inplace=True, kind=\"stable\")\n",
    "\n",
    "field_names = user_fields + movie_fields\n",
    "\n",
    "df_data = df_data[field_names].reset_index(drop=True)\n",
    "print(\"Total number after filtering:\", len(df_data))\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the feature dict for CTR data\n",
    "\n",
    "def add_to_dict(dict, feature):\n",
    "    if feature not in dict:\n",
    "        dict[feature] = len(dict)\n",
    "\n",
    "feature_dict = {field : {} for field in field_names}\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    for field in field_names:\n",
    "        add_to_dict(feature_dict[field], row[field])\n",
    "\n",
    "user_feat_count = [len(feature_dict[field]) for field in user_fields]\n",
    "item_feat_count = [len(feature_dict[field]) for field in movie_fields]\n",
    "\n",
    "# Treat user and movie features differently\n",
    "user_feat_offset, item_feat_offset = [0], [0]\n",
    "for c in user_feat_count[:-1]:\n",
    "    user_feat_offset.append(user_feat_offset[-1] + c)\n",
    "\n",
    "for c in item_feat_count[:-1]:\n",
    "    item_feat_offset.append(item_feat_offset[-1] + c)\n",
    "\n",
    "print(\"---------------------------------------------------------------\")\n",
    "for f, fc, fo in zip(user_fields, user_feat_count, user_feat_offset):\n",
    "    print(f, fc, fo)\n",
    "    \n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "for f, fc, fo in zip(movie_fields, item_feat_count, item_feat_offset):\n",
    "    print(f, fc, fo)\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "for field in field_names:\n",
    "    df_data[field] = df_data[field].apply(lambda x: feature_dict[field][x])\n",
    "\n",
    "df_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_feat_dict = {}\n",
    "\n",
    "for _, row in tqdm(df_data.iterrows()):\n",
    "    if row[\"Movie ID\"] not in movie_feat_dict:\n",
    "        movie_feat_dict[row[\"Movie ID\"]] = [int(row[\"Movie ID\"]), int(row[\"Movie title\"]), int(row[\"Movie genre\"])]\n",
    "\n",
    "movie_feats_table = [movie_feat_dict[i] for i in range(item_feat_count[0])]\n",
    "print(len(movie_feats_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history_dict = {\n",
    "    \"ID\": {k: [] for k in set(df_data[\"User ID\"])},\n",
    "}\n",
    "\n",
    "user_history_column = {\n",
    "    \"ID\": [],\n",
    "}\n",
    "\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    user_id, movie_id = row[\"User ID\"], row[\"Movie ID\"]\n",
    "    user_history_column[\"ID\"].append(user_history_dict[\"ID\"][user_id].copy())\n",
    "    user_history_dict[\"ID\"][user_id].append(movie_id)\n",
    "\n",
    "df_data[\"user history ID\"] = user_history_column[\"ID\"]\n",
    "\n",
    "df_data = df_data[df_data[\"user history ID\"].apply(lambda x: len(x)) >= 5].reset_index(drop=True)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx = []\n",
    "test_idx = []\n",
    "\n",
    "for uid, df_u in tqdm(df_data.groupby([\"User ID\"])):\n",
    "    valid_idx.append(df_u.tail(2).index[0])\n",
    "    test_idx.append(df_u.tail(1).index[0])\n",
    "\n",
    "valid_idx = sorted(valid_idx)\n",
    "test_idx = sorted(test_idx)\n",
    "train_idx = sorted(list(set(range(len(df_data))) - set(valid_idx + test_idx)))\n",
    "\n",
    "df_train = df_data.iloc[train_idx].reset_index(drop=True)\n",
    "df_valid = df_data.iloc[valid_idx].reset_index(drop=True)\n",
    "df_test = df_data.iloc[test_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = len(df_train)\n",
    "valid_num = len(df_valid)\n",
    "test_num = len(df_test)\n",
    "print(\"Num train/valid/test:\", train_num, valid_num, test_num)\n",
    "\n",
    "df_train.to_parquet(os.path.join(target_dir, \"train.parquet.gz\"), compression=\"gzip\")\n",
    "df_valid.to_parquet(os.path.join(target_dir, \"valid.parquet.gz\"), compression=\"gzip\")\n",
    "df_test.to_parquet(os.path.join(target_dir, \"test.parquet.gz\"), compression=\"gzip\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_to_users = {i: [] for i in range(item_feat_count[0])}\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    movie_to_users[row[\"Movie ID\"]].append(row[\"User ID\"])\n",
    "\n",
    "\n",
    "movie_to_users = [list(set(movie_to_users[i])) for i in range(item_feat_count[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = {\n",
    "    \"user_fields\": user_fields,\n",
    "    \"item_fields\": movie_fields,\n",
    "    \"user_feat_count\": user_feat_count,\n",
    "    \"item_feat_count\": item_feat_count,\n",
    "    \"user_feat_offset\": user_feat_offset,\n",
    "    \"item_feat_offset\": item_feat_offset,\n",
    "    \"movie_feats_table\": movie_feats_table,\n",
    "    \"feature_dict\": feature_dict,\n",
    "    \"item_to_users\": movie_to_users\n",
    "}\n",
    "\n",
    "json.dump(meta_data, open(os.path.join(target_dir, \"match-meta.json\"), \"w\"), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "df_data = pd.concat([df_train, df_valid, df_test]).reset_index(drop=True)\n",
    "\n",
    "user_X = []\n",
    "item_X = []\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    user_X.append([row[field] for field in user_fields])\n",
    "    item_X.append([row[field] for field in movie_fields])\n",
    "\n",
    "hist_ID = df_data[\"user history ID\"].tolist()\n",
    "hist_length = [len(x) for x in hist_ID]\n",
    "\n",
    "user_X = np.array(user_X)\n",
    "item_X = np.array(item_X)\n",
    "\n",
    "hist_ID = pad_sequence(\n",
    "    [torch.tensor(x[-30:]) for x in hist_ID], \n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "hist_mask = pad_sequence(\n",
    "    [torch.ones(min(x, 30)) for x in hist_length], \n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "print(\"user_X\", user_X.shape)\n",
    "print(\"item_X\", item_X.shape)\n",
    "print(\"hist_ID\", hist_ID.shape)\n",
    "print(\"hist_mask\", hist_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f\"match.h5\"), \"w\") as hf:\n",
    "    hf.create_dataset(\"train user data\", data=user_X[:train_num, :])\n",
    "    hf.create_dataset(\"valid user data\", data=user_X[train_num:train_num+valid_num, :])\n",
    "    hf.create_dataset(\"test user data\", data=user_X[train_num+valid_num:, :])\n",
    "\n",
    "    hf.create_dataset(\"train item data\", data=item_X[:train_num, :])\n",
    "    hf.create_dataset(\"valid item data\", data=item_X[train_num:train_num+valid_num, :])\n",
    "    hf.create_dataset(\"test item data\", data=item_X[train_num+valid_num:, :])\n",
    "\n",
    "    hf.create_dataset(\"train history ID\", data=hist_ID[:train_num, :])\n",
    "    hf.create_dataset(\"valid history ID\", data=hist_ID[train_num:train_num+valid_num, :])\n",
    "    hf.create_dataset(\"test history ID\", data=hist_ID[train_num+valid_num:, :])\n",
    "\n",
    "    hf.create_dataset(\"train history mask\", data=hist_mask[:train_num, :])\n",
    "    hf.create_dataset(\"valid history mask\", data=hist_mask[train_num:train_num+valid_num, :])\n",
    "    hf.create_dataset(\"test history mask\", data=hist_mask[train_num+valid_num:, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huawei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
