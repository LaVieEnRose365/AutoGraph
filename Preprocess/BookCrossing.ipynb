{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from transformers import set_seed\n",
    "import hashlib\n",
    "import json\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "dataset = \"BookCrossing\"\n",
    "source_dir = os.path.join(f\"Datasets/{dataset}\", \"raw_data\")\n",
    "target_dir = os.path.join(f\"data/{dataset}\", \"proc_data\")\n",
    "os.makedirs(target_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters, digits\n",
    "\n",
    "def character_check(item, special_letters=\"\"):\n",
    "    for letter in str(item):\n",
    "        if letter not in ascii_letters + digits + special_letters:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read user info\n",
    "\n",
    "user_fields = [\"User ID\", \"Location\", \"Age\"]\n",
    "pattern = re.compile(r'NULL|\".*?(?<!\\\\)\"', re.S)\n",
    "with open(os.path.join(source_dir, \"BX-Users.csv\"), 'r', encoding='cp1252') as f:\n",
    "    content = pattern.findall(f.read())\n",
    "    content = [s[1:-1] if s != 'NULL' else None for s in content]\n",
    "    processed_list = list(np.array(content).reshape((-1, 3)))\n",
    "    processed_list.pop(0)\n",
    "    df_users = pd.DataFrame(processed_list, columns=user_fields)\n",
    "\n",
    "# There are messy info/code (or totally empty) in the `Location` field, we only use the country instead.\n",
    "# E.g., ['&#37073;&#24030;&#26159;, &#20013;&#22269;&#27827;&#21335;&#30465;&#37073;&#24030;&#24066;, china', \n",
    "#        'philippine science high school - cmc, mcc main stadium, sagadan, tubod, lanao del norte, philippines', \n",
    "#        '6.a.4.a.6.a`4.a, 6.a.4.a.6.a`4.a.6.a`4.a.6.a.4.a.6.a`4.aoe6.a`4.a -- 6.a.4.a.6.a`4.aoe6.a`4.a ã??, ä¸\\xadå?½']\n",
    "def convert_location_to_country(x):\n",
    "    x = x.split(', ')[-1].strip().title().replace(\"!\", \"\").strip()\n",
    "    if x.lower() in [\"usa\", \"us\", \"u s\", \"u s a\"]:\n",
    "        x = \"USA\"\n",
    "    if x.lower() in [\"uk\", \"u k\"]:\n",
    "        x = \"UK\"\n",
    "    while len(x) > 0 and x[-1] in [\",\", \".\"]:\n",
    "        x = x[:-1]\n",
    "    while len(x) > 0 and x[0] in [\",\", \".\"]:\n",
    "        x = x[1:]\n",
    "    if \"U.S\" in x.upper() and x != \"U.S. Virgin Islands\":\n",
    "        x = \"USA\"\n",
    "    if x in [\"San José\", \"San Josï¿½\"]:\n",
    "        x = \"USA\"\n",
    "    if x in [\"España\", \"Castilla-León\", \"Espaã±A\", \"Cataluña\", \"Mérida\", \"Álava\", \"Málaga\", \"A Coruña\", \"Barcelonès\", \"Berguedà\",\n",
    "              \"Espaï¿½A\", \"Castilla-Leï¿½N\", \"A Coruï¿½A\", \"Cataluï¿½A\", \"Barcelonï¿½S\", \"Ï¿½Lava\", \"Mï¿½Rida\", \"Berguedï¿½\", \"Mï¿½Laga\"] or \"spain\" in x.lower():\n",
    "        x = \"Spain\"\n",
    "    if x in [\"L`Italia\"]:\n",
    "        x = \"Italy\"\n",
    "    if x in [\"Baden-Württemberg\", \"Bademn Würtemberg\", \"Baden-Wï¿½Rttemberg\", \"Bademn Wï¿½Rtemberg\"]:\n",
    "        x = \"German\"\n",
    "    if x in [\"Cote D`Ivoire\", \"Côte D\", \"Cï¿½Te D\"]:\n",
    "        x = \"Ivory Coast\"\n",
    "    if x in [\"Oberösterreich\", \"Oberï¿½Sterreich\"]:\n",
    "        x = \"Austria\"\n",
    "    if x in [\"México\", \"Mï¿½Xico\"]:\n",
    "        x = \"Mexico\"\n",
    "    if x in [\"Türkiye\", \"Içel\", \"Tï¿½Rkiye\"]:\n",
    "        x = \"Turkey\"\n",
    "    if x in [\"L`Algérie\", \"Algérie\", \"Kärnten\", \"Kï¿½Rnten\", \"L`Algï¿½Rie\", \"Algï¿½Rie\"]:\n",
    "        x = \"Algeria\"\n",
    "    if \"Brasil\" in x:\n",
    "        x = \"Brazil\"\n",
    "    if x in [\"Rhône-Alpes\", \"Rhône Alpes\", \"Rhï¿½Ne-Alpes\", \"Rhï¿½Ne Alpes\"]:\n",
    "        x = \"France\"\n",
    "    if \"Greece\" in x:\n",
    "        x = \"Greece\"\n",
    "    if x in [\"Santarém\", \"Santarï¿½M\"]:\n",
    "        x = \"Portugal\"\n",
    "    if x in [\"Länsi-Suomen Lääni\", \"Lï¿½Nsi-Suomen Lï¿½Ï¿½Ni\"]:\n",
    "        x = \"Finland\"\n",
    "    if x in [\"V.Götaland\", \"Nyhamnsläge\", \"V.Gï¿½Taland\", \"Nyhamnslï¿½Ge\"]:\n",
    "        x = \"Sweden\"\n",
    "    if x in [\"Moçambique\", \"Moï¿½Ambique\"]:\n",
    "        x = \"Mozambique\"\n",
    "    if x in [\"Ix Región\", \"Ix Regiï¿½N\"]:\n",
    "        x = \"Chile\"\n",
    "    if x in [\"Maï¿½Opolskie\", \"Ma³Opolskie\"]:\n",
    "        x = \"Poland\"\n",
    "    if x in [\"Perï¿½\", \"Perãº\"]:\n",
    "        x = \"Peru\"\n",
    "    if x != \"China\" and (\"china\" in x.lower() or x == \"La Chine Éternelle\" or x == \"La Chine Ï¿½Ternelle\"):\n",
    "        x = \"China\"\n",
    "    if x == \"Ï¿½Ï¿½Ï¿½\":\n",
    "        x = \"China\"\n",
    "    if (x == \"\" or \\\n",
    "        x in [\"Öð¹Ú\", \"ºþäï\", \"We`Re Global\", \"Ï¿½Ï¿½Ï¿½Ï¿½\", \"Iï¿½El\"] or \\\n",
    "        len(x) == 1 or \\\n",
    "        \"N/A\" in x or \\\n",
    "        \"&#\" in x or \\\n",
    "        \"?\" in x or \\\n",
    "        \"@\" in x or \\\n",
    "        \"*\" in x):\n",
    "        x = \"unknown\"\n",
    "    return x\n",
    "df_users[\"Location\"] = df_users[\"Location\"].apply(convert_location_to_country)\n",
    "df_users[\"location_check\"] = df_users[\"Location\"].apply(lambda x: character_check(x, special_letters=\"- .&/()\"))\n",
    "\n",
    "assert len(df_users.loc[df_users[\"location_check\"] == 1, \"Location\"]) == 0\n",
    "\n",
    "# Nearly a half of the features in `Age` field are missing.\n",
    "def convert_age_to_bucket(x):\n",
    "    if x is None:\n",
    "        x = \"unknown\"\n",
    "    else:\n",
    "        x = int(x)\n",
    "        # There are out-of-range ages (e.g., < 5 or > 100).\n",
    "        if x < 5 or x > 100:\n",
    "            x = \"unknown\"\n",
    "        # Age discretization\n",
    "        elif x < 18:\n",
    "            x = \"under 18\"\n",
    "        elif 18 <= x < 25:\n",
    "            x = \"18-24\"\n",
    "        elif 25 <= x < 30:\n",
    "            x = \"25-29\"\n",
    "        elif 30 <= x < 35:\n",
    "            x = \"30-34\"\n",
    "        elif 35 <= x < 40:\n",
    "            x = \"35-39\"\n",
    "        elif 40 <= x < 45:\n",
    "            x = \"40-44\"\n",
    "        elif 45 <= x < 50:\n",
    "            x = \"45-49\"\n",
    "        elif 50 <= x < 55:\n",
    "            x = \"50-54\"\n",
    "        elif 55 <= x < 60:\n",
    "            x = \"55-59\"\n",
    "        else:\n",
    "            x = \"60+\"\n",
    "    return x\n",
    "df_users[\"Age\"] = df_users[\"Age\"].apply(convert_age_to_bucket)\n",
    "\n",
    "for field in user_fields:\n",
    "    for s in list(df_users[field]):\n",
    "        if field == \"User ID\":\n",
    "            assert 1 <= int(s) <= 278858\n",
    "        if field == \"Location\":\n",
    "            assert 2 <= len(s) <= 45\n",
    "        if field == \"Age\":\n",
    "            assert s in [\"unknown\", \"under 18\" ,\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60+\"]\n",
    "\n",
    "df_users = df_users[user_fields]\n",
    "print(df_users.head())\n",
    "print('---------------------------------------------------------------')\n",
    "print(df_users.info())\n",
    "print('---------------------------------------------------------------')\n",
    "print(df_users.describe())\n",
    "print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read book info\n",
    "\n",
    "book_fields = [\"ISBN\", \"Book title\", \"Author\", \"Publication year\", \"Publisher\"]\n",
    "pattern = re.compile(r'(?<=\");(?=\")')\n",
    "processed_list = []\n",
    "with open(os.path.join(source_dir, \"BX-Books.csv\"), 'r', encoding='cp1252') as f:\n",
    "    for line in f.readlines():\n",
    "        split_line = pattern.split(line.strip())\n",
    "        split_line = [item[1:-1].strip('\\t') for item in split_line][:-3] # The last three image URLs are not needed.\n",
    "        processed_list.append(split_line)\n",
    "    processed_list.pop(0)\n",
    "    df_books = pd.DataFrame(processed_list, columns=book_fields)\n",
    "\n",
    "# ISBN should only contain letters and digits.\n",
    "df_books['ISBN_check'] = df_books['ISBN'].apply(lambda x: character_check(x))\n",
    "df_books = df_books[df_books['ISBN_check'] == 0]\n",
    "\n",
    "# There are invalid publication years, i.e., \"0\"\n",
    "def convert_publication_year(x):\n",
    "    x = x if len(x) == 4 else \"unknown\"\n",
    "    return x\n",
    "df_books[\"Publication year\"] = df_books[\"Publication year\"].apply(convert_publication_year)\n",
    "\n",
    "df_books[\"Publisher\"] = df_books[\"Publisher\"].apply(lambda x: x if x.lower() != \"n/a\" else \"unknown\")\n",
    "df_books[\"Author\"] = df_books[\"Author\"].apply(lambda x: x if x.lower() != \"n/a\" else \"unknown\")\n",
    "\n",
    "for field in book_fields:\n",
    "    for s in list(df_books[field]):\n",
    "        if field == \"ISBN\":\n",
    "            assert len(s) == 10\n",
    "        if field == \"Book title\":\n",
    "            assert 1 <= len(s) <= 256\n",
    "        if field == \"Author\":\n",
    "            assert 1 <= len(s) <= 143\n",
    "        if field == \"Publication year\":\n",
    "            assert s == \"unknown\" or len(s) == 4\n",
    "        if field == \"Publisher\":\n",
    "            assert 1 <= len(s) <= 134\n",
    "\n",
    "df_books = df_books[book_fields]\n",
    "print(df_books.head())\n",
    "print('---------------------------------------------------------------')\n",
    "print(df_books.info())\n",
    "print('---------------------------------------------------------------')\n",
    "print(df_books.describe())\n",
    "print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ratings\n",
    "\n",
    "processed_list = []\n",
    "with open(os.path.join(source_dir, \"BX-Book-Ratings.csv\"), 'r', encoding='cp1252') as f:\n",
    "    for line in f.readlines():\n",
    "        split_line = line.strip().split(';')\n",
    "        split_line = [item[1:-1] for item in split_line]\n",
    "        processed_list.append(split_line)\n",
    "    processed_list.pop(0)\n",
    "\n",
    "df_ratings = pd.DataFrame(processed_list, columns=[\"User ID\", \"ISBN\", \"rating\"])\n",
    "print(\"Total number of ratings:\", len(df_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.merge(df_ratings, df_users, on=[\"User ID\"], how=\"inner\")\n",
    "df_data = pd.merge(df_data, df_books, on=[\"ISBN\"], how=\"inner\")\n",
    "\n",
    "df_data[\"rating\"] = df_data[\"rating\"].apply(lambda x: int(x))\n",
    "df_data = df_data[df_data[\"rating\"] > 5]\n",
    "\n",
    "field_names = user_fields + book_fields\n",
    "\n",
    "df_data = df_data[field_names].reset_index(drop=True)\n",
    "print(\"Total number after filtering:\", len(df_data))\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the feature dict\n",
    "\n",
    "def add_to_dict(dict, feature):\n",
    "    if feature not in dict:\n",
    "        dict[feature] = len(dict)\n",
    "\n",
    "field_names = user_fields + book_fields\n",
    "feature_dict = {field : {} for field in field_names}\n",
    "\n",
    "for idx, row in df_data.iterrows():\n",
    "    for field in field_names:\n",
    "        add_to_dict(feature_dict[field], row[field])\n",
    "\n",
    "user_feat_count = [len(feature_dict[field]) for field in user_fields]\n",
    "item_feat_count = [len(feature_dict[field]) for field in book_fields]\n",
    "\n",
    "\n",
    "# Treat user and book features differently\n",
    "user_feat_offset, item_feat_offset = [0], [0]\n",
    "for c in user_feat_count[:-1]:\n",
    "    user_feat_offset.append(user_feat_offset[-1] + c)\n",
    "\n",
    "for c in item_feat_count[:-1]:\n",
    "    item_feat_offset.append(item_feat_offset[-1] + c)\n",
    "\n",
    "print(\"---------------------------------------------------------------\")\n",
    "for f, fc, fo in zip(user_fields, user_feat_count, user_feat_offset):\n",
    "    print(f, fc, fo)\n",
    "    \n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "for f, fc, fo in zip(book_fields, item_feat_count, item_feat_offset):\n",
    "    print(f, fc, fo)\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "for field in field_names:\n",
    "    df_data[field] = df_data[field].apply(lambda x: feature_dict[field][x])\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_feat_dict = {}\n",
    "for _, row in tqdm(df_data.iterrows()):\n",
    "    if row[\"ISBN\"] not in book_feat_dict:\n",
    "        book_feat_dict[row[\"ISBN\"]] = [int(row[\"ISBN\"]), int(row[\"Book title\"]), int(row[\"Author\"]), int(row[\"Publication year\"]), int(row[\"Publisher\"])]\n",
    "\n",
    "book_feat_table = [book_feat_dict[i] for i in range(len(book_feat_dict))]\n",
    "print(len(book_feat_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history_dict = {\n",
    "    \"ID\": {k: [] for k in set(df_data[\"User ID\"])},\n",
    "}\n",
    "\n",
    "user_history_column = {\n",
    "    \"ID\": [],\n",
    "}\n",
    "\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    user_id, movie_id = row[\"User ID\"], row[\"ISBN\"]\n",
    "    user_history_column[\"ID\"].append(user_history_dict[\"ID\"][user_id].copy())\n",
    "    user_history_dict[\"ID\"][user_id].append(movie_id)\n",
    "\n",
    "df_data[\"user history ID\"] = user_history_column[\"ID\"]\n",
    "\n",
    "df_data = df_data[df_data[\"user history ID\"].apply(lambda x: len(x)) >= 5].reset_index(drop=True)\n",
    "\n",
    "# 7-core filtering\n",
    "user_counter = df_data[\"User ID\"].value_counts()\n",
    "user_counter = user_counter[user_counter >= 3]\n",
    "df_data = df_data[df_data[\"User ID\"].isin(user_counter.index)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx = []\n",
    "test_idx = []\n",
    "\n",
    "for uid, df_u in tqdm(df_data.groupby([\"User ID\"])):\n",
    "    valid_idx.append(df_u.tail(2).index[0])\n",
    "    test_idx.append(df_u.tail(1).index[0])\n",
    "\n",
    "valid_idx = sorted(valid_idx)\n",
    "test_idx = sorted(test_idx)\n",
    "train_idx = sorted(list(set(range(len(df_data))) - set(valid_idx + test_idx)))\n",
    "\n",
    "df_train = df_data.iloc[train_idx].reset_index(drop=True)\n",
    "df_valid = df_data.iloc[valid_idx].reset_index(drop=True)\n",
    "df_test = df_data.iloc[test_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = len(df_train)\n",
    "valid_num = len(df_valid)\n",
    "test_num = len(df_test)\n",
    "print(\"Num train/valid/test:\", train_num, valid_num, test_num)\n",
    "\n",
    "df_train.to_parquet(os.path.join(target_dir, \"train.parquet.gz\"), compression=\"gzip\")\n",
    "df_valid.to_parquet(os.path.join(target_dir, \"valid.parquet.gz\"), compression=\"gzip\")\n",
    "df_test.to_parquet(os.path.join(target_dir, \"test.parquet.gz\"), compression=\"gzip\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_to_users = {i: [] for i in range(len(book_feat_dict))}\n",
    "\n",
    "for idx, row in tqdm(df_train.iterrows()):\n",
    "    for isbn in row[\"user history ID\"] + [row[\"ISBN\"]]:\n",
    "        book_to_users[isbn].append(row[\"User ID\"])\n",
    "\n",
    "book_to_users = [list(set(book_to_users[i])) for i in range(len(book_feat_dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = {\n",
    "    \"user_fields\": user_fields,\n",
    "    \"item_fields\": book_fields,\n",
    "    \"user_feat_count\": user_feat_count,\n",
    "    \"item_feat_count\": item_feat_count,\n",
    "    \"user_feat_offset\": user_feat_offset,\n",
    "    \"item_feat_offset\": item_feat_offset,\n",
    "    \"book_feats_table\": book_feat_table,\n",
    "    \"feature_dict\": feature_dict,\n",
    "    \"item_to_users\": book_to_users\n",
    "}\n",
    "\n",
    "json.dump(meta_data, open(os.path.join(target_dir, \"match-meta.json\"), \"w\"), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "df_data = pd.concat([df_train, df_valid, df_test]).reset_index(drop=True)\n",
    "\n",
    "user_X = []\n",
    "item_X = []\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    user_X.append([row[field] for field in user_fields])\n",
    "    item_X.append([row[field] for field in book_fields])\n",
    "\n",
    "hist_ID = df_data[\"user history ID\"].tolist()\n",
    "hist_length = [len(x) for x in hist_ID]\n",
    "\n",
    "user_X = np.array(user_X)\n",
    "item_X = np.array(item_X)\n",
    "\n",
    "hist_ID = pad_sequence(\n",
    "    [torch.tensor(x[-30:]) for x in hist_ID], \n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "hist_mask = pad_sequence(\n",
    "    [torch.ones(min(x, 30)) for x in hist_length], \n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "print(\"user_X\", user_X.shape)\n",
    "print(\"item_X\", item_X.shape)\n",
    "print(\"hist_ID\", hist_ID.shape)\n",
    "print(\"hist_mask\", hist_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f\"match.h5\"), \"w\") as hf:\n",
    "    hf.create_dataset(\"train user data\", data=user_X[:train_num, :])\n",
    "    hf.create_dataset(\"valid user data\", data=user_X[train_num:train_num+valid_num, :])\n",
    "    hf.create_dataset(\"test user data\", data=user_X[train_num+valid_num:, :])\n",
    "\n",
    "    hf.create_dataset(\"train item data\", data=item_X[:train_num, :])\n",
    "    hf.create_dataset(\"valid item data\", data=item_X[train_num:train_num+valid_num, :])\n",
    "    hf.create_dataset(\"test item data\", data=item_X[train_num+valid_num:, :])\n",
    "\n",
    "    hf.create_dataset(\"train history ID\", data=hist_ID[:train_num, :])\n",
    "    hf.create_dataset(\"valid history ID\", data=hist_ID[train_num:train_num+valid_num, :])\n",
    "    hf.create_dataset(\"test history ID\", data=hist_ID[train_num+valid_num:, :])\n",
    "\n",
    "    hf.create_dataset(\"train history mask\", data=hist_mask[:train_num, :])\n",
    "    hf.create_dataset(\"valid history mask\", data=hist_mask[train_num:train_num+valid_num, :])\n",
    "    hf.create_dataset(\"test history mask\", data=hist_mask[train_num+valid_num:, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
